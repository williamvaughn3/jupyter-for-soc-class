{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Spark SQL Tables via Pyspark**\n",
    "----------------------------------------------------------------------------\n",
    "## Goals:\n",
    "* Practice Spark SQL via PySpark skills\n",
    "* Ensure JupyterLab Server, Spark Cluster & Elasticsearch are communicating\n",
    "* Practice Query execution via Pyspark\n",
    "* Create template for future queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import SparkSession Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a SparkSession instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HELK Reader\") \\\n",
    "    .master(\"spark://helk-spark-master:7077\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from the HELK Elasticsearch via Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_reader = (spark.read\n",
    "    .format(\"org.elasticsearch.spark.sql\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"es.read.field.as.array.include\", \"tags\")\n",
    "    .option(\"es.nodes\",\"helk-elasticsearch:9200\")\n",
    "    .option(\"es.net.http.auth.user\",\"elastic\")\n",
    ")\n",
    "    #PLEASE REMEMBER!!!!\n",
    "    #If you are using elastic TRIAL license, then you need the es.net.http.auth.pass config option set\n",
    "    #Example: .option(\"es.net.http.auth.pass\",\"elasticpassword\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Sysmon Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 4 ms, total: 4 ms\n",
      "Wall time: 1.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sysmon_df = es_reader.load(\"logs-endpoint-winevent-sysmon-*/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Sysmon SQL temporary View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sysmon_df.createOrReplaceTempView(\"sysmon_events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run SQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sysmon_ps_execution = spark.sql(\n",
    "    '''\n",
    "    SELECT event_id,process_parent_name,process_name\n",
    "    FROM sysmon_events\n",
    "    WHERE event_id = 1\n",
    "        AND process_name = \"powershell.exe\"\n",
    "        AND NOT process_parent_name = \"explorer.exe\"\n",
    "    '''\n",
    ")\n",
    "sysmon_ps_execution.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sysmon_ps_module = spark.sql(\n",
    "    '''\n",
    "    SELECT event_id,process_name\n",
    "    FROM sysmon_events\n",
    "    WHERE event_id = 7 \n",
    "        AND (\n",
    "            lower(file_description) = \"system.management.automation\"\n",
    "            OR lower(module_loaded) LIKE \"%\\\\\\\\system.management.automation%\"\n",
    "        ) \n",
    "    '''\n",
    ")\n",
    "sysmon_ps_module.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sysmon_ps_pipe = spark.sql(\n",
    "    '''\n",
    "    SELECT event_id,process_name\n",
    "    FROM sysmon_events\n",
    "    WHERE event_id = 17\n",
    "        AND lower(pipe_name) LIKE \"\\\\\\\\pshost%\"\n",
    "    '''\n",
    ")\n",
    "sysmon_ps_pipe.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read PowerShell Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "powershell_df = es_reader.load(\"logs-endpoint-winevent-powershell-*/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register PowerShell SQL temporary View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "powershell_df.createOrReplaceTempView(\"powershell_events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_named_pipe = spark.sql(\n",
    "    '''\n",
    "    SELECT event_id\n",
    "    FROM powershell_events\n",
    "    WHERE event_id = 53504\n",
    "    '''\n",
    ")\n",
    "ps_named_pipe.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark_Python3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
